{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes and Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes algorithms are a family of powerful and easy-to-train classifiers that determine the probability of an outcome given a set of conditions using Bayes' theorem.\n",
    "\n",
    "#### We are going to discuss the following:\n",
    "\n",
    "1.  Bayes' theorem and its applications\n",
    "2. Naive Bayes classifiers (Bernoulli, Multinomial, and Gaussian)\n",
    "3.  Discriminant analysis (both linear and quadratic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classification \n",
    "\n",
    "1. Supervised Learning Algorithm used for classfication\n",
    "2. It is based on Bayes Theorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Bayes Theorum ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P(A \\enspace/ \\enspace B) =  \\large \\frac{ P (B \\enspace/\\enspace A) \\, P (A) } { P(B) } $\n",
    "\n",
    "$ P(A) $ :  **Priori Probabality** [Probablity of event before event B]\n",
    "\n",
    "$ P(A / B) $ :  **Posterior Probabality** [Probablity of event after event B is true]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the general discrete case, the formula can be re-expressed considering all possible outcomes for the random variable A:\n",
    "- $ P(A / B) =  \\large \\frac{ P (B \\enspace/ \\enspace A) \\, P (A) }  {\\sum_{i} P(B \\enspace / \\enspace A_i) P( A_i ) }  $\n",
    "\n",
    "- As the denominator is a normalization factor, the formula is often expressed as a proportionality relationship:\n",
    "    - $ P(A \\enspace / \\enspace B) \\propto { P (B \\enspace / \\enspace A) \\, P (A) } $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we want to implement a very simple spam filter and we've collected 100 emails. We know that 30 are spam and that 70 are regular. So, we can say that P(Spam) = 0.3.\n",
    "\n",
    "However, we'd like to evaluate using some criteria (for simplicity, let's consider a single one)—for example, email text is shorter than 50 characters. Therefore, our query becomes the following:\n",
    "\n",
    "$ P( Spam \\enspace/ \\enspace Text < 50 char) =  \\large \\frac{ P (Text < 50 char \\enspace/  \\enspace Spam)  \\, P (Spam) } { P(Text < 50 char) } $\n",
    "\n",
    "Let's suppose that 35 emails have text shorter than 50 characters, so P(Text < 50 chars) = 0.35. Looking only into our spam folder, we discover that only 25 spam emails have short text, so that P(Text < 50 chars|Spam) = 25/30 = 0.83. The result is this:\n",
    "\n",
    "$ P(Spam \\enspace / \\enspace Text < 50 chars) =  \\large \\frac{ 0.83 \\enspace* \\enspace 0.3 }  {0.35 }  $\n",
    "\n",
    "So, after receiving a very short email, there is a 71% probability that it's spam. Now, we can understand the role of P(Text < 50 chars|Spam); as we have actual data, we can measure how probable is our hypothesis given the query.\n",
    "\n",
    "In other words, we have defined a likelihood (compare this concept with the logistic regression), which is a weight between the Apriori probability and the A Posteriori one:\n",
    "\n",
    " $ \\large {P(A \\enspace / \\enspace B) \\propto {Likelihood \\, P (A) }} $ \n",
    " \n",
    " $ \\large {P_{APosterior} \\propto {Likelihood * P_{APriori} }} $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Text                          | category   |\n",
    "|-------------------------------|------------|\n",
    "| A great game                  | Sports     |\n",
    "| the election was over         | Non Sports |\n",
    "| very clean match              | Sports     |\n",
    "| a clean but forgettable game  | Sports     |\n",
    "| it was a close election       | Non Sports |\n",
    "\n",
    "- Using the Naive Bayes we will try to classify :  \" A very close game \" should  be in sports or non sports category.\n",
    "- So in terms of probablity we have to find :\n",
    "    - P(Sports / \"A very close game \" ) and P(Non sports / \"A very close game \" ) \n",
    "    - To find the probablity we have to find probality of ever word:\n",
    "        - P(A) * P(\"very\") * P(close) * P(game)\n",
    "    - Get probablity of each when Sports is True \n",
    "        - P(\"A very close game\"/Sports) = P(A/Sports) * P(very/Sports) * P(close/Sports) * P(game/Sports)\n",
    "    - Get probablity of each when Non Sports is True \n",
    "        - P(\"A very close game\"/Non Sports) = P(A/Non Sports) * P(very/Non Sports) * P(close/Non Sports) * P(game/Non Sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P(A/sports) = 2/11 : \n",
    "    1.How many time A is there in catergory sports ? 2\n",
    "    2.How many words are there is category sports ? 11\n",
    "\n",
    "### P(very/Sports) = 1/11 : \n",
    "    Ask the above questions \n",
    "\n",
    "### P(close/Sports) : 0/11\n",
    "    1. This is important because the word close is not there in the Sports category\n",
    "    2. It cannot be 0/11 else it will make the entire probality to be zero .\n",
    "    3. Here we use **Laplace smoothing** : its a difficult formula but verry easy to understand\n",
    "    \n",
    "$\\large{\\hat\\theta_i= \\frac{x_i + \\alpha}{N + \\alpha d} } \\qquad (i=1,\\ldots,d)$\n",
    "    \n",
    "$\\large x_i =$  word count , How many time A is there in catergory sports ?\n",
    "    \n",
    "$\\large N= $ total number of words in that catergory.\n",
    "    \n",
    "$\\large d= $ total number of unique words in **all the categories**.\n",
    "  \n",
    "$\\large \\alpha  = 1  $ \n",
    "    \n",
    "    - The default value for α is 1.0 (in this case, it's called Laplace factor) and it prevents the model from setting null probabilities when the frequency is zero\n",
    "    \n",
    "    - When α < 1.0, it's usually called the **Lidstone factor**. Clearly, if α → 0, the effect becomes more and more negligible, returning to a scenario very similar to the Bernoulli Naive Bayes. In our example, we're going to consider the default value of 1.0.\n",
    "    \n",
    "    - So this eqaution can be represted as :\n",
    "    \n",
    "$P(words) = \\large {\\frac {word \\enspace count \\enspace + \\enspace 1} {total \\enspace no \\enspace of \\enspace words \\enspace + \\enspace no \\enspace of \\enspace unique \\enspace  words}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this lets find the probablity of every word :\n",
    "\n",
    "P(a/sports) = (  2 + 1 )/ (11 + 14) \n",
    "\n",
    "P(close/sports) = ( 0 + 1 / (11 + 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Text           | P(word/sports)  | P(word / non sports )  |\n",
    "|----------------|-----------------|------------------------|\n",
    "| A              | 2+1/11+14       | 1+1/9+14               |\n",
    "| Very           | 1+1/11+14       | 0+1/9+14               |\n",
    "| close          | 0+1/11+14       | 1+1/9+14               |\n",
    "| game           | 2+1/11+14       | 0+1/9+14               |\n",
    "\n",
    "\n",
    "- Get probablity of each when Sports is True \n",
    "    - P(\"A very close game\"/Sports) = P(A/Sports) * P(very/Sports) * P(close/Sports) * P(game/Sports)\n",
    "    - P(\"A very close game\"/Sports) = 4.61 * $ 10^{-5} $  \n",
    "        \n",
    "- Get probablity of each when Non Sports is True \n",
    "    - P(\"A very close game\"/Non Sports) = P(A/Non Sports) * P(very/Non Sports) * P(close/Non Sports) * P(game/Non Sports)\n",
    "    - P(\"A very close game\"/Non Sports) = 1.43 * $ 10^{-5} $\n",
    "        \n",
    "### Conclusion\n",
    "- P(\"A very close game\"/Sports)  has a higher probablity so this sentence belongs to the sports category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-learn implements three Naive Bayes variants based on the same number of different probabilistic distributions: \n",
    "    1. Bernoulli : Is a binary distribution, and is useful when a feature can be present or absent.\n",
    "    2. Multinomial : Is a discrete distribution and is used whenever a feature must be represented by a whole number (for example, in NLP, it can be the frequency of a term).\n",
    "    3. Gaussian : Is a continuous distribution characterized by its mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n",
    "\n",
    "- Where it is used ? \n",
    "    - If you have a dataset where the features value is Binary \n",
    "        - 1 or 0 \n",
    "        - True or Flase \n",
    "        - Positive or negative\n",
    "        - Yes or NO\n",
    "        - Success or Failure \n",
    "- Bernoulli Naive bayes uses Bernoulli distribution :\n",
    "    - P(success) = p\n",
    "    - P(failure) = q = 1-p\n",
    "    - X is a random variable , any column or datatype in your dataset which is binary.\n",
    "    - X = 1 [success]\n",
    "    - X = 0 [failure]\n",
    "    - We can now say X has a Bernoulli distribution .\n",
    "    - Bernoulli distribution explained by maths :\n",
    "        - (X=x) = $ p^x(1-p)^{1-x} $ \n",
    "            - X is a random variable\n",
    "            - x is 1 or 0\n",
    "            - Subsituting 1 or 0 in the above we will achieve P(success) or P(failure)\n",
    "            - The above can be represented as \n",
    "                - P(X) = p : if X=1 , \n",
    "                - P(X) = q : if X=0\n",
    "- Multivariate Bernoulli model or Bernoulli model :  It generates an indicator for each term of the vocabulary, either 1 indicating presence of the term in the document or 0 indicating absence.\n",
    "- When classifying a test document, the Bernoulli model uses binary occurrence information, ignoring the number of occurrences, whereas the multinomial model keeps track of multiple occurrences. As a result, the Bernoulli model typically makes many mistakes when classifying long documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A multinomial distribution is useful to model feature vectors where each value represents, for example, the number of occurrences of a term or its relative frequency. If the feature vectors have n elements and each of them can assume k different values with probability pk.\n",
    "\n",
    "- The conditional probabilities P(x(i)|yj) are computed with a frequency count (which corresponds to applying a maximum likelihood approach), but in this case, it's important to consider also a correction parameter α (called Laplace or Lidstone smoothing factor), to avoid null probabilities:\n",
    "\n",
    "- Where it can be used ?\n",
    "    - To find the number of occurance/frequency/count of a word in a document.\n",
    "    - The discrete count is given\n",
    "- Multinomial Distribution : \n",
    "    - P($x_1,x_2,x_3,x_4.....x_n$)= $\\large\\frac{n!} {x_1!x_2!x_3!....x_n!}$ $P_{1}^{x_1}$...$P_{k}^{x_k}$  ....Lets break the maths into understandable peices\n",
    "    - n : Size of you random sample\n",
    "    - $x_{1}$....$x_{n}$ : Represent the number of occurance of $x_{1}$...$x_{n}$ in the random sample.\n",
    "    - $P_{1}^{x_1}$...$P_{k}^{x_k}$ : Represent the Probablity of $x_{1}$...$x_{n}$ in the random sample.\n",
    "    - The Last two points are little confusing : Lets understand with a dataset \n",
    "    - The below table is the probablity of blood group in the entire data set .\n",
    "    \n",
    "| BG | O   | A   | B   | AB   |\n",
    "|----|-----|-----|-----|------|\n",
    "| P  | .44 | .42 | .10 | .004 |\n",
    "                 \n",
    "What is the probablity in a given random sample of 6 people have count 1:O , 2:A, 2:B , 1:AB.\n",
    "\n",
    "- $x_{1}$....$x_{n}$ : $x_{1}$=1 , $x_{2}$=2 , $x_{3}$=2 , $x_{4}$=1 : it is the count of people given the problem\n",
    "- $P_{1}^{x_1}$...$P_{k}^{x_k}$ :     $P_{1}^{x_1 = 1:O}=.44^{1}$ ,   $P_{1}^{x_2=2:A}=.42^{2}$  ,  $P_{1}^{x_3=2:B}= .10^{2}$ ,  $P_{1}^{x_4=1:AB}=.004^{1}$\n",
    "- n : The number of random sample\n",
    "- P($x_1,x_2,x_3,x_4.....x_n$) = $\\large {\\frac {6!} {1!  2!  2!  1!}}  0.44^{1} .42^{2} .10^{2} .004^{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gaussian Naive Bayes is useful when working with continuous values whose probabilities can be modeled using Gaussian distributions .\n",
    "- Gussian or Normal distribution :\n",
    "    - $p(x=v \\mid C_k)=\\large \\frac{1}{\\large \\sqrt{2\\pi\\sigma^2_k}}\\,e^{ -\\large\\frac{(v-\\mu_k)^2}{2 \\large \\sigma^2_k} }$ \n",
    "    - Probablity of v in $C_{k}$\n",
    "    - ${\\sigma}$ : Standard deviation\n",
    "    - ${\\sigma^{2}}$ : Variance\n",
    "    - ${\\mu_k}$ : Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
